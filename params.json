{"name":"Seosuite","tagline":"Automated Search Engine Optimization Testing Tool","body":"SEO Tool Suite\r\n==============\r\n\r\nRecluse is a suite of SEO tools for discovering and investigating common SEO issues across a domain.\r\n\r\n- **SEO Crawler** crawls a site and saves the results to the db.\r\n- **SEO Linter** checks HTML against common SEO rules.\r\n- **SEO Reporter** outputs the crawl reports in various formats.\r\n- **SEO Dashboard** displays the crawl data for the browser.\r\n\r\n\r\nSEO Crawler\r\n===========\r\n\r\nSEO Crawler takes a given url and continues to crawl the internal links on the site until it can't find new urls to crawl. The crawler saves common request information like status code and content type. It also runs the SEO Linter to check for common SEO issues. The results of a crawl are saved to a database for later investigation and a JUnit report is saved to the file system. The idea is to catch basic, common errors across a whole site and allow for investigation into edge cases.\r\n\r\nInstructions\r\n------------\r\n\r\n    > pip install -r requirements.txt\r\n    > cat schema.sql | mysql -uusername -p\r\n    > cp testurls.example testurls\r\n    > cp config.yaml.example config.yaml\r\n    > ./seocrawler.py http://fashionista.com\r\n\r\nCLI Options\r\n-----------\r\n\r\n    $> python seocrawler.py --help\r\n    Usage: seocrawler.py [options]\r\n    \r\n    Crawl the given url(s) and check them for SEO or navigation problems.\r\n    \r\n    Options:\r\n      -h, --help            show this help message and exit\r\n      -i, --internal        Crawl any internal link urls that are found in the\r\n                            content of the page.\r\n      --user-agent=USER_AGENT\r\n                            The user-agent string to request pages with.\r\n      --delay=DELAY         The number of milliseconds to delay between each\r\n                            request.\r\n      --database=DATABASE   A yaml configuration file with the database\r\n                            configuration properties.\r\n      -o OUTPUT, --output=OUTPUT\r\n                            The path of the file where the output junix xml will\r\n                            be written to.\r\n    \r\n      Input Options:\r\n        -f FILE, --file=FILE\r\n                            A file containing a list of urls (one url per line) to\r\n                            process.\r\n        -u BASE_URL, --base_url=BASE_URL\r\n                            A single url to use as a starting point for crawling.\r\n        -r RUN_ID, --run_id=RUN_ID\r\n                            The id from a previous run to resume.\r\n        -y YAML, --yaml=YAML\r\n                            A yaml file containing a list of urls to process. The\r\n                            yaml file should have a section labeled\r\n                            \"seocrawlerurls\" that contains a list of the urls to\r\n                            crawl.\r\n\r\nSEO Linter\r\n==========\r\n\r\nThe SEO Linter tests a given string for common SEO rules. The levels (CRITICAL, ERROR, WARN and INFO) are not hard, fast SEO rules but should help in an SEO investigation.\r\n\r\nInstructions\r\n------------\r\n\r\n    > pip install -r requirements.txt\r\n    > curl --silent http://fashionista.com?__escaped_fragment__= | python seolinter/__init__.py\r\n\r\nLint Rules\r\n----------\r\n\r\n- E02: has title (ERROR)\r\n- W03: title < 58 chars (WARN)\r\n- E05: has meta description (ERROR)\r\n- W06: meta description < 150 chars (WARN)\r\n- E08: has canonical (ERROR)\r\n- E09: has h1 (ERROR)\r\n- I10: missing rel=prev (INFO)\r\n- I11: missing rel=next (INFO)\r\n- E12: title matches <1 h1 word (ERROR)\r\n- E13: title matches <1 meta description word (ERROR)\r\n- W14: title matches <3 h1 words (WARN)\r\n- W15: title matches <3 meta description word (WARN)\r\n- W16: <300 outlinks on page (WARN)\r\n- E17: <1000 outlinks on page (ERROR)\r\n- W18: size < 200K (WARN)\r\n- W19: all img tags have alt attribute (WARN)\r\n- I20: has robots=nofollow (INFO)\r\n- I21: has robots=noindex (INFO)\r\n- C22: has head (CRITICAL)\r\n- W23: h1 count > 1 (WARN)\r\n\r\n\r\nSEO Reporter\r\n============\r\n\r\nThe SEO Reporter takes a run_id, gets report data, then outputs it in a given format. The main use-case is running reports in Jenkins with JUnit. By default, it outputs the latest run as junit.\r\n\r\nInstructions\r\n------------\r\n\r\n    > pip install -r requirements.txt\r\n    # python seoreporter/__init__.py [type] [format] [run_id]\r\n    > python seoreporter/__init__.py build junit d09b8571-5c8a-42ff-8ab7-c38f4f8871c4\r\n\r\n\r\nSEO Dashboard\r\n=============\r\n\r\nThe SEO Dashboard presents a table of crawl data as a table viewable in the browser.\r\n\r\nInstructions\r\n------------\r\n\r\n    > pip install -r requirements.txt\r\n    > python seodashboard/main.py\r\n    > open localhost:5000\r\n\r\n\r\nMIT License\r\n===========\r\n\r\nCopyright (c) 2014 Say Media Ltd\r\n\r\nPermission is hereby granted, free of charge, to any person\r\nobtaining a copy of this software and associated documentation\r\nfiles (the \"Software\"), to deal in the Software without\r\nrestriction, including without limitation the rights to use,\r\ncopy, modify, merge, publish, distribute, sublicense, and/or sell\r\ncopies of the Software, and to permit persons to whom the\r\nSoftware is furnished to do so, subject to the following\r\nconditions:\r\n\r\nThe above copyright notice and this permission notice shall be\r\nincluded in all copies or substantial portions of the Software.\r\n\r\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\r\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\r\nOF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\r\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\r\nHOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\r\nWHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\r\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\r\nOTHER DEALINGS IN THE SOFTWARE.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}